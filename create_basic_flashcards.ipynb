{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Data Extraction And Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex\n",
    "import pandas as pd\n",
    "import spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_raw_matches(file_source: str):\n",
    "    regex_pattern = \"(^\\d{1,4} (?!\\|))((.*\\n){0,8}?.*?(?=\\d+ \\|))\"\n",
    "    with open(file_source, \"r\") as data:\n",
    "        string = data.read()\n",
    "\n",
    "    unfiltered_matches = regex.findall(regex_pattern, string, regex.MULTILINE)\n",
    "    unfiltered_matches.pop(0)\n",
    "\n",
    "    filtered_matches = []\n",
    "    next_match = 1\n",
    "    for match in unfiltered_matches:\n",
    "        if int(match[0]) == next_match:\n",
    "            filtered_matches.append(match)\n",
    "            next_match += 1\n",
    "\n",
    "    return filtered_matches\n",
    "\n",
    "\n",
    "matches = extract_raw_matches(\"source.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_raw_matches_into_dict(matches):\n",
    "    result = []\n",
    "    for match in matches:\n",
    "        result.append(\n",
    "            {\n",
    "                \"frequency_idx\": int(match[0]),\n",
    "                \"raw_match\": regex.sub(\"\\d+\", \"\", match[1].replace(\"|\", \"\")),\n",
    "            }\n",
    "        )\n",
    "    return result\n",
    "\n",
    "\n",
    "match_dict = parse_raw_matches_into_dict(matches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_raw_sentence(dictionary):\n",
    "    str_to_parse = dictionary[\"raw_match\"].replace(\"æ\", \"œ\").replace(\"’\", \"'\")\n",
    "    part_of_speach_codes = {\n",
    "        \"adj\",\n",
    "        \"adji\",\n",
    "        \"adji\\(pl\\)\",\n",
    "        \"adv\",\n",
    "        \"conj\",\n",
    "        \"det\",\n",
    "        \"intj\",\n",
    "        \"n\",\n",
    "        \"nf\",\n",
    "        \"nm\",\n",
    "        \"nadj\",\n",
    "        \"prep\",\n",
    "        \"pro\",\n",
    "        \"v\",\n",
    "        \"nmi\",\n",
    "        \"nfi\",\n",
    "        \"nmpl\",\n",
    "        \"nfpl\",\n",
    "        \"adj\\(f\\)\",\n",
    "        \"nadj\\(f\\)\",\n",
    "        \"nm\\(pl\\)\",\n",
    "        \"nf\\(pl\\)\",\n",
    "        \"adj\\(pl\\)\",\n",
    "        \"nadj\\(pl\\)\",\n",
    "        \"nmfi\",\n",
    "        \"adjf\",\n",
    "        \"nadjpl\",\n",
    "    }\n",
    "\n",
    "    min = None\n",
    "    max = None\n",
    "    for i in part_of_speach_codes:\n",
    "        part_of_speach_regex = f\" {i}[, ]\"\n",
    "        match = regex.search(part_of_speach_regex, str_to_parse, regex.MULTILINE)\n",
    "        if match:\n",
    "            start, end = match.span()\n",
    "            if min is None or start < min:\n",
    "                min = start\n",
    "            if max is None or end > max:\n",
    "                max = end\n",
    "\n",
    "    if min is None and max is None:\n",
    "        end_of_first_word = str_to_parse.find(\" \")\n",
    "        min, max = end_of_first_word, end_of_first_word\n",
    "\n",
    "    dictionary[\"french_word\"] = str_to_parse[:min].strip()\n",
    "    dictionary[\"pos_codes\"] = str_to_parse[min:max].strip()\n",
    "    dictionary[\"word_english\"] = str_to_parse[max:].split(\"\\n\")[0]\n",
    "    split_sentence = (\n",
    "        str_to_parse[max:]\n",
    "        .replace(dictionary[\"word_english\"], \"\", 1)\n",
    "        .replace(\"\\n\", \"\")\n",
    "        .split(\"–\")\n",
    "    )\n",
    "    if len(split_sentence) == 1:\n",
    "        split_sentence = (\n",
    "            str_to_parse[max:]\n",
    "            .replace(dictionary[\"word_english\"], \"\", 1)\n",
    "            .replace(\"\\n\", \"\")\n",
    "            .split(\"-\")\n",
    "        )\n",
    "    sentence_french = split_sentence[0].strip()\n",
    "    sentence_english = \"-\".join(split_sentence[1:]).strip()\n",
    "    dictionary[\"sentence_french\"], dictionary[\"sentence_english\"] = (\n",
    "        sentence_french,\n",
    "        sentence_english,\n",
    "    )\n",
    "    return dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "failures = []\n",
    "for dict in match_dict:\n",
    "    try:\n",
    "        parse_raw_sentence(dict)\n",
    "    except Exception as e:\n",
    "        failures.append({\"frequency_idx\": dict[\"frequency_idx\"], \"exception\": str(e)})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Replace Target Word In Example Sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"fr_core_news_lg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_word_in_sentence(word, sentence):\n",
    "    word_regex = \"(?<=(\\.|,|\\?| |^|'|-))\" + word + \"(?=(\\.|,|\\?| |$||\\!|-))\"\n",
    "    new_sentence = regex.sub(word_regex, \"___\", sentence)\n",
    "    return new_sentence\n",
    "\n",
    "\n",
    "def delete_lemma_from_example(word, sentence):\n",
    "    doc = nlp(sentence)\n",
    "    word_doc = nlp(word)\n",
    "    input_word_token = word_doc[0]\n",
    "    tokens = [\n",
    "        {\n",
    "            \"original\": token.text,\n",
    "            \"lemma\": token.lemma_,\n",
    "            \"similarity\": token.similarity(input_word_token),\n",
    "        }\n",
    "        for token in doc\n",
    "    ]\n",
    "    tokens.sort(key=lambda x: x[\"similarity\"], reverse=True)\n",
    "    for token in tokens:\n",
    "        if token[\"lemma\"] == word or token[\"original\"] == word:\n",
    "            return True, replace_word_in_sentence(token[\"original\"], sentence)\n",
    "\n",
    "    if len(word_doc) == 1:\n",
    "        return False, replace_word_in_sentence(tokens[0][\"original\"], sentence)\n",
    "    else:\n",
    "        return False, replace_word_in_sentence(word, sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mw/ts7zr6qn7x1cbg2pvqvp3sh40000gn/T/ipykernel_21818/3212104019.py:10: UserWarning: [W008] Evaluating Token.similarity based on empty vectors.\n",
      "  tokens = [{\"original\": token.text, \"lemma\": token.lemma_, 'similarity': token.similarity(input_word_token)} for token in doc]\n"
     ]
    }
   ],
   "source": [
    "no_lemma_match = []\n",
    "replace_failures = []\n",
    "for item in match_dict:\n",
    "    word = item[\"french_word\"]\n",
    "    sentence = item[\"sentence_french\"]\n",
    "    success, result = delete_lemma_from_example(word, sentence)\n",
    "\n",
    "    item[\"sentence_french_deleted\"] = result\n",
    "\n",
    "    if not success:\n",
    "        no_lemma_match.append(item)\n",
    "    if \"_\" not in result:\n",
    "        replace_failures.append(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in match_dict:\n",
    "    item[\"pronunciation\"] = f\"[sound:french_audio_{item['frequency_idx']}.mp3]\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_df = pd.DataFrame(match_dict)\n",
    "export_df.drop(inplace=True, columns=\"raw_match\")\n",
    "export_df.to_csv(\"basic_french_flashcards.csv\", index=False, sep=\"&\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('french_flashcards')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0a00106c57f919c93c04c3e5bc403e8c74a6df6712ad57de0b788fdb8e17cb9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
