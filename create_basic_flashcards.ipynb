{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Data Extraction And Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex\n",
    "import pandas as pd\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_raw_matches(file_source: str):\n",
    "    regex_pattern = '(^\\d{1,4} (?!\\|))((.*\\n){0,8}?.*?(?=\\d+ \\|))'\n",
    "    with open(file_source, 'r') as data:\n",
    "        string = data.read()\n",
    "\n",
    "    unfiltered_matches = regex.findall(regex_pattern, string, regex.MULTILINE)\n",
    "    unfiltered_matches.pop(0)\n",
    "\n",
    "    filtered_matches = []\n",
    "    next_match = 1\n",
    "    for match in unfiltered_matches:\n",
    "        if int(match[0]) == next_match:\n",
    "            filtered_matches.append(match)\n",
    "            next_match +=1\n",
    "\n",
    "    return filtered_matches\n",
    "\n",
    "matches = extract_raw_matches('source.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_raw_matches_into_dict(matches):\n",
    "    result = []\n",
    "    for match in matches:\n",
    "        result.append({'frequency_idx': int(match[0]), \"raw_match\": regex.sub('\\d+', \"\", match[1].replace(\"|\", \"\"))})\n",
    "    return result\n",
    "match_dict = parse_raw_matches_into_dict(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_raw_sentence(dictionary):\n",
    "    str_to_parse = dictionary['raw_match'].replace('æ','œ').replace('’', \"'\")\n",
    "    part_of_speach_codes = {'adj', 'adji','adji\\(pl\\)', 'adv', 'conj', 'det', 'intj', 'n', 'nf', 'nm', 'nadj', 'prep', 'pro', 'v', 'nmi', 'nfi', 'nmpl', 'nfpl', 'adj\\(f\\)', 'nadj\\(f\\)', 'nm\\(pl\\)', 'nf\\(pl\\)', 'adj\\(pl\\)', 'nadj\\(pl\\)', 'nmfi', \"adjf\", \"nadjpl\"  }\n",
    "    \n",
    "    min = None\n",
    "    max = None\n",
    "    for i in part_of_speach_codes:\n",
    "        part_of_speach_regex =f' {i}[, ]'\n",
    "        match =  regex.search(part_of_speach_regex, str_to_parse,regex.MULTILINE)\n",
    "        if match:\n",
    "            start, end = match.span()\n",
    "            if min is None or start < min:\n",
    "                min = start\n",
    "            if max is None or end > max:\n",
    "                max = end\n",
    "    \n",
    "    if min is None and max is None:\n",
    "        end_of_first_word = str_to_parse.find(' ')\n",
    "        min, max = end_of_first_word, end_of_first_word\n",
    "            \n",
    "    dictionary['french_word'] = str_to_parse[:min].strip()\n",
    "    dictionary['pos_codes'] = str_to_parse[min:max].strip()\n",
    "    dictionary['word_english'] = str_to_parse[max:].split('\\n')[0]\n",
    "    split_sentence = str_to_parse[max:].replace(dictionary['word_english'], \"\", 1).replace('\\n', \"\").split('–')\n",
    "    if len(split_sentence) == 1:\n",
    "        split_sentence =  str_to_parse[max:].replace(dictionary['word_english'], \"\", 1).replace('\\n', \"\").split('-')\n",
    "    sentence_french = split_sentence[0].strip()\n",
    "    sentence_english = \"-\".join(split_sentence[1:]).strip()\n",
    "    dictionary['sentence_french'], dictionary[\"sentence_english\"] = sentence_french, sentence_english\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "failures = []\n",
    "for dict in match_dict:\n",
    "    try:\n",
    "        parse_raw_sentence(dict)\n",
    "    except Exception as e:\n",
    "        failures.append({'frequency_idx': dict['frequency_idx'], 'exception': str(e)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Replace Target Word In Example Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('fr_core_news_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_word_in_sentence(word, sentence):\n",
    "    word_regex = \"(?<=(\\.|,|\\?| |^|'|-))\" + word + \"(?=(\\.|,|\\?| |$||\\!|-))\"\n",
    "    new_sentence = regex.sub(word_regex, \"___\", sentence)\n",
    "    return new_sentence\n",
    "\n",
    "def delete_lemma_from_example(word, sentence):\n",
    "    doc = nlp(sentence)\n",
    "    word_doc = nlp(word)\n",
    "    input_word_token = word_doc[0]\n",
    "    tokens = [{\"original\": token.text, \"lemma\": token.lemma_, 'similarity': token.similarity(input_word_token)} for token in doc]\n",
    "    tokens.sort(key=lambda x: x[\"similarity\"],reverse=True)\n",
    "    for token in tokens:\n",
    "        if token[\"lemma\"] == word or token[\"original\"] == word:\n",
    "            return True, replace_word_in_sentence(token[\"original\"], sentence)\n",
    "    \n",
    "    if len(word_doc) == 1:\n",
    "        return False, replace_word_in_sentence(tokens[0]['original'], sentence)\n",
    "    else:\n",
    "        return False, replace_word_in_sentence(word, sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mw/ts7zr6qn7x1cbg2pvqvp3sh40000gn/T/ipykernel_10701/3212104019.py:10: UserWarning: [W008] Evaluating Token.similarity based on empty vectors.\n",
      "  tokens = [{\"original\": token.text, \"lemma\": token.lemma_, 'similarity': token.similarity(input_word_token)} for token in doc]\n"
     ]
    }
   ],
   "source": [
    "no_lemma_match = []\n",
    "replace_failures = []\n",
    "for item in match_dict:\n",
    "    word = item['french_word']\n",
    "    sentence = item['sentence_french']\n",
    "    success, result = delete_lemma_from_example(word, sentence)\n",
    "\n",
    "    item['sentence_french_deleted'] = result\n",
    "    \n",
    "    if not success:\n",
    "        no_lemma_match.append(item)\n",
    "    if \"_\" not in result:\n",
    "        replace_failures.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in match_dict:\n",
    "    item['pronunciation'] = f\"[sound:french_audio_{item['frequency_idx']}.mp3]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_df = pd.DataFrame(match_dict)\n",
    "export_df.drop(inplace=True, columns='raw_match')\n",
    "export_df.to_csv('basic_french_flashcards.csv', index=False, sep='&')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Download Audio Sound Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import FORVO_API_KEY, SCRAPER_API_KEY\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import urllib.parse\n",
    "import sysc\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_sound_file(dictionary, session, api_key = FORVO_API_KEY,):\n",
    "    word = urllib.parse.quote(dictionary[\"french_word\"])\n",
    "    freq_idx = dictionary[\"frequency_idx\"]\n",
    "    url =  f\"https://apifree.forvo.com/key/{api_key}/format/json/action/standard-pronunciation/word/{word}/language/fr\"\n",
    "    file_name = f\"french_audio_{freq_idx}.mp3\"\n",
    "    path = \"mp3_files/\"\n",
    "    try:\n",
    "        async with session.get(url) as response:\n",
    "            json_response = await response.json()\n",
    "            mp3_download_url = json_response[\"items\"][0][\"pathmp3\"]\n",
    "        async with session.get(mp3_download_url) as mp3_response:\n",
    "            mp3_file = await mp3_response.read()\n",
    "        if sys.getsizeof(mp3_file) < 100:\n",
    "            raise ValueError('No File Downloaded')\n",
    "        with open(f'{path}{file_name}', 'wb') as local_file:\n",
    "            local_file.write(mp3_file)\n",
    "            print(f'Downloaded {freq_idx}')\n",
    "    except Exception as e:\n",
    "        return (dictionary, str(e))\n",
    "    \n",
    "\n",
    "\n",
    "async def get_multiple_sound_files(match_dict):\n",
    "    all_responses = []\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        chunked_match_dicts = [match_dict[i:i + 5] for i in range(0, len(match_dict), 5)]\n",
    "        for chunked_match_dict in chunked_match_dicts:\n",
    "            tasks = []\n",
    "            tasks.append(asyncio.sleep(5))\n",
    "            for item in chunked_match_dict:\n",
    "                tasks.append(get_sound_file(item, session))\n",
    "            responses = await asyncio.gather(*tasks)\n",
    "            all_responses.extend([response for response in responses if response is not None])\n",
    "    return all_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try_again = await get_multiple_sound_files(match_dict[2130:2380])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_failed_downloads(dir):\n",
    "    mp3_files = os.listdir('./mp3_files/')\n",
    "    failed_downloads = [file for file in mp3_files if os.path.getsize(f'{dir}{file}') <= 200]\n",
    "    failed_downloads_idx = [int(filename.split(\"_\")[2].split('.')[0]) - 1 for filename in failed_downloads]\n",
    "    failed_donwloads_dict_items = [match_dict[i] for i in failed_downloads_idx]\n",
    "    return failed_donwloads_dict_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_donwloads_dict_items = get_failed_downloads(\"./mp3_files/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Download Verb Conjugations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Dataframe of Verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(match_dict)\n",
    "is_verb = lambda x: True if regex.search('(?<=( |^))v(?=( |$))', x) else False\n",
    "df_verbs = df[df.pos_codes.apply(is_verb)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_verb_conjugation(word, session):\n",
    "    url = f'https://www.wordreference.com/conj/frverbs.aspx?v={word}'\n",
    "    params = {'api_key': SCRAPER_API_KEY, 'url': url}\n",
    "    endpoint = \"http://api.scraperapi.com\"\n",
    "    async with session.get(endpoint, params = params) as response:\n",
    "        if response.status != 200:\n",
    "            print(response)\n",
    "            raise ValueError('Invalid HTTP status')\n",
    "        response_text =  await response.text()\n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Returns True if conjugation table does not contain any items with the css class 'antiquated'\n",
    "\n",
    "def find_antiquated_conjugation_table(table):\n",
    "    iterable = table.descendants\n",
    "    for i in iterable:\n",
    "        if type(i) == bs4.element.Tag and 'class' in i.attrs and 'antiquated' in i['class']: \n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parses HTML response, removes all tables with antiquated conjugations, returns list of records with all relevant conjugations\n",
    "\n",
    "def process_conjugation_response(response_text):\n",
    "    soup = BeautifulSoup(response_text, \"html5lib\")\n",
    "    conjugation_tables = soup.find_all('table', 'neoConj')\n",
    "    relevant_conjugation_tables = list(filter(find_antiquated_conjugation_table, conjugation_tables))\n",
    "    parsed_conjugation_tables = [pd.read_html(str(table), flavor='html5lib')[0] for table in relevant_conjugation_tables]\n",
    "    parsed_conjugation_dicts = [table.to_dict('records') for table in parsed_conjugation_tables]\n",
    "    return parsed_conjugation_dicts\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "async with aiohttp.ClientSession() as session:\n",
    "    response_text = await get_verb_conjugation('parler', session)\n",
    "    parsed_conjugation_records = process_conjugation_response(response_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'présent': 'je', 'présent.1': 'parle'}, {'présent': 'tu', 'présent.1': 'parles'}, {'présent': 'il, elle, on', 'présent.1': 'parle'}, {'présent': 'nous', 'présent.1': 'parlons'}, {'présent': 'vous', 'présent.1': 'parlez'}, {'présent': 'ils, elles', 'présent.1': 'parlent'}], [{'imparfait': 'je', 'imparfait.1': 'parlais'}, {'imparfait': 'tu', 'imparfait.1': 'parlais'}, {'imparfait': 'il, elle, on', 'imparfait.1': 'parlait'}, {'imparfait': 'nous', 'imparfait.1': 'parlions'}, {'imparfait': 'vous', 'imparfait.1': 'parliez'}, {'imparfait': 'ils, elles', 'imparfait.1': 'parlaient'}], [{'passé simple': 'je', 'passé simple.1': 'parlai'}, {'passé simple': 'tu', 'passé simple.1': 'parlas'}, {'passé simple': 'il, elle, on', 'passé simple.1': 'parla'}, {'passé simple': 'nous', 'passé simple.1': 'parlâmes'}, {'passé simple': 'vous', 'passé simple.1': 'parlâtes'}, {'passé simple': 'ils, elles', 'passé simple.1': 'parlèrent'}], [{'futur simple': 'je', 'futur simple.1': 'parlerai'}, {'futur simple': 'tu', 'futur simple.1': 'parleras'}, {'futur simple': 'il, elle, on', 'futur simple.1': 'parlera'}, {'futur simple': 'nous', 'futur simple.1': 'parlerons'}, {'futur simple': 'vous', 'futur simple.1': 'parlerez'}, {'futur simple': 'ils, elles', 'futur simple.1': 'parleront'}], [{'passé composé': \"j'\", 'passé composé.1': 'ai parlé'}, {'passé composé': 'tu', 'passé composé.1': 'as parlé'}, {'passé composé': 'il, elle, on', 'passé composé.1': 'a parlé'}, {'passé composé': 'nous', 'passé composé.1': 'avons parlé'}, {'passé composé': 'vous', 'passé composé.1': 'avez parlé'}, {'passé composé': 'ils, elles', 'passé composé.1': 'ont parlé'}], [{'plus-que-parfait': \"j'\", 'plus-que-parfait.1': 'avais parlé'}, {'plus-que-parfait': 'tu', 'plus-que-parfait.1': 'avais parlé'}, {'plus-que-parfait': 'il, elle, on', 'plus-que-parfait.1': 'avait parlé'}, {'plus-que-parfait': 'nous', 'plus-que-parfait.1': 'avions parlé'}, {'plus-que-parfait': 'vous', 'plus-que-parfait.1': 'aviez parlé'}, {'plus-que-parfait': 'ils, elles', 'plus-que-parfait.1': 'avaient parlé'}], [{'futur antérieur': \"j'\", 'futur antérieur.1': 'aurai parlé'}, {'futur antérieur': 'tu', 'futur antérieur.1': 'auras parlé'}, {'futur antérieur': 'il, elle, on', 'futur antérieur.1': 'aura parlé'}, {'futur antérieur': 'nous', 'futur antérieur.1': 'aurons parlé'}, {'futur antérieur': 'vous', 'futur antérieur.1': 'aurez parlé'}, {'futur antérieur': 'ils, elles', 'futur antérieur.1': 'auront parlé'}], [{'présent': \"que je/j'\", 'présent.1': 'parle'}, {'présent': 'que tu', 'présent.1': 'parles'}, {'présent': \"qu'il, elle, on\", 'présent.1': 'parle'}, {'présent': 'que nous', 'présent.1': 'parlions'}, {'présent': 'que vous', 'présent.1': 'parliez'}, {'présent': \"qu'ils, elles\", 'présent.1': 'parlent'}], [{'passé': \"que je/j'\", 'passé.1': 'aie parlé'}, {'passé': 'que tu', 'passé.1': 'aies parlé'}, {'passé': \"qu'il, elle, on\", 'passé.1': 'ait parlé'}, {'passé': 'que nous', 'passé.1': 'ayons parlé'}, {'passé': 'que vous', 'passé.1': 'ayez parlé'}, {'passé': \"qu'ils, elles\", 'passé.1': 'aient parlé'}], [{'présent': \"je/j'\", 'présent.1': 'parlerais'}, {'présent': 'tu', 'présent.1': 'parlerais'}, {'présent': 'il, elle, on', 'présent.1': 'parlerait'}, {'présent': 'nous', 'présent.1': 'parlerions'}, {'présent': 'vous', 'présent.1': 'parleriez'}, {'présent': 'ils, elles', 'présent.1': 'parleraient'}], [{'passé': \"je/j'\", 'passé.1': 'aurais parlé'}, {'passé': 'tu', 'passé.1': 'aurais parlé'}, {'passé': 'il, elle, on', 'passé.1': 'aurait parlé'}, {'passé': 'nous', 'passé.1': 'aurions parlé'}, {'passé': 'vous', 'passé.1': 'auriez parlé'}, {'passé': 'ils, elles', 'passé.1': 'auraient parlé'}], [{'présent': nan, 'présent.1': '–'}, {'présent': '(tu)', 'présent.1': 'parle !'}, {'présent': nan, 'présent.1': '–'}, {'présent': '(nous)', 'présent.1': 'parlons !'}, {'présent': '(vous)', 'présent.1': 'parlez !'}, {'présent': nan, 'présent.1': '–'}]]\n"
     ]
    }
   ],
   "source": [
    "parsed_conjugation_records = process_conjugation_response(result)\n",
    "print(parsed_conjugation_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_form': 'parler',\n",
       " 'présent_je': 'parle',\n",
       " 'présent_tu': 'parlerais',\n",
       " 'présent_il_elle_on': 'parlerait',\n",
       " 'présent_nous': 'parlerions',\n",
       " 'présent_vous': 'parleriez',\n",
       " 'présent_ils_elles': 'parleraient',\n",
       " 'imparfait_je': 'parlais',\n",
       " 'imparfait_tu': 'parlais',\n",
       " 'imparfait_il_elle_on': 'parlait',\n",
       " 'imparfait_nous': 'parlions',\n",
       " 'imparfait_vous': 'parliez',\n",
       " 'imparfait_ils_elles': 'parlaient',\n",
       " 'passé_simple_je': 'parlai',\n",
       " 'passé_simple_tu': 'parlas',\n",
       " 'passé_simple_il_elle_on': 'parla',\n",
       " 'passé_simple_nous': 'parlâmes',\n",
       " 'passé_simple_vous': 'parlâtes',\n",
       " 'passé_simple_ils_elles': 'parlèrent',\n",
       " 'futur_simple_je': 'parlerai',\n",
       " 'futur_simple_tu': 'parleras',\n",
       " 'futur_simple_il_elle_on': 'parlera',\n",
       " 'futur_simple_nous': 'parlerons',\n",
       " 'futur_simple_vous': 'parlerez',\n",
       " 'futur_simple_ils_elles': 'parleront',\n",
       " \"passé_composé_j'\": 'ai parlé',\n",
       " 'passé_composé_tu': 'as parlé',\n",
       " 'passé_composé_il_elle_on': 'a parlé',\n",
       " 'passé_composé_nous': 'avons parlé',\n",
       " 'passé_composé_vous': 'avez parlé',\n",
       " 'passé_composé_ils_elles': 'ont parlé',\n",
       " \"plus-que-parfait_j'\": 'avais parlé',\n",
       " 'plus-que-parfait_tu': 'avais parlé',\n",
       " 'plus-que-parfait_il_elle_on': 'avait parlé',\n",
       " 'plus-que-parfait_nous': 'avions parlé',\n",
       " 'plus-que-parfait_vous': 'aviez parlé',\n",
       " 'plus-que-parfait_ils_elles': 'avaient parlé',\n",
       " \"futur_antérieur_j'\": 'aurai parlé',\n",
       " 'futur_antérieur_tu': 'auras parlé',\n",
       " 'futur_antérieur_il_elle_on': 'aura parlé',\n",
       " 'futur_antérieur_nous': 'aurons parlé',\n",
       " 'futur_antérieur_vous': 'aurez parlé',\n",
       " 'futur_antérieur_ils_elles': 'auront parlé',\n",
       " \"présent_que_je/j'\": 'parle',\n",
       " 'présent_que_tu': 'parles',\n",
       " \"présent_qu'il_elle_on\": 'parle',\n",
       " 'présent_que_nous': 'parlions',\n",
       " 'présent_que_vous': 'parliez',\n",
       " \"présent_qu'ils_elles\": 'parlent',\n",
       " \"passé_que_je/j'\": 'aie parlé',\n",
       " 'passé_que_tu': 'aies parlé',\n",
       " \"passé_qu'il_elle_on\": 'ait parlé',\n",
       " 'passé_que_nous': 'ayons parlé',\n",
       " 'passé_que_vous': 'ayez parlé',\n",
       " \"passé_qu'ils_elles\": 'aient parlé',\n",
       " \"présent_je/j'\": 'parlerais',\n",
       " \"passé_je/j'\": 'aurais parlé',\n",
       " 'passé_tu': 'aurais parlé',\n",
       " 'passé_il_elle_on': 'aurait parlé',\n",
       " 'passé_nous': 'aurions parlé',\n",
       " 'passé_vous': 'auriez parlé',\n",
       " 'passé_ils_elles': 'auraient parlé',\n",
       " 'présent_(tu)': 'parle !',\n",
       " 'présent_(nous)': 'parlons !',\n",
       " 'présent_(vous)': 'parlez !'}"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_to_result_dictionary(records, word):\n",
    "    result = {'base_form': word}\n",
    "    for record in records:\n",
    "        for dict in record:\n",
    "            dict_as_lst = list(dict.items())\n",
    "            tense = dict_as_lst[0][0]\n",
    "            person = dict_as_lst[0][1]\n",
    "            conjugation = dict_as_lst[1][1]\n",
    "            if not isinstance(tense, str) or not isinstance(person, str) or not isinstance(conjugation, str):\n",
    "                continue\n",
    "            format_key = lambda x: x.replace(',', '').replace(' ', '_')\n",
    "            key = format_key(f'{tense}_{person}')\n",
    "            result[key] = conjugation\n",
    "    return result\n",
    "\n",
    "convert_to_result_dictionary(parsed_conjugation_records, 'parler')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('french_flashcards')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0a00106c57f919c93c04c3e5bc403e8c74a6df6712ad57de0b788fdb8e17cb9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
