{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Data Extraction And Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex\n",
    "import pandas as pd\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_raw_matches(file_source: str):\n",
    "    regex_pattern = '(^\\d{1,4} (?!\\|))((.*\\n){0,8}?.*?(?=\\d+ \\|))'\n",
    "    with open(file_source, 'r') as data:\n",
    "        string = data.read()\n",
    "\n",
    "    unfiltered_matches = regex.findall(regex_pattern, string, regex.MULTILINE)\n",
    "    unfiltered_matches.pop(0)\n",
    "\n",
    "    filtered_matches = []\n",
    "    next_match = 1\n",
    "    for match in unfiltered_matches:\n",
    "        if int(match[0]) == next_match:\n",
    "            filtered_matches.append(match)\n",
    "            next_match +=1\n",
    "\n",
    "    return filtered_matches\n",
    "\n",
    "matches = extract_raw_matches('source.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_raw_matches_into_dict(matches):\n",
    "    result = []\n",
    "    for match in matches:\n",
    "        result.append({'frequency_idx': int(match[0]), \"raw_match\": regex.sub('\\d+', \"\", match[1].replace(\"|\", \"\"))})\n",
    "    return result\n",
    "match_dict = parse_raw_matches_into_dict(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_raw_sentence(dictionary):\n",
    "    str_to_parse = dictionary['raw_match'].replace('æ','œ').replace('’', \"'\")\n",
    "    part_of_speach_codes = {'adj', 'adji','adji\\(pl\\)', 'adv', 'conj', 'det', 'intj', 'n', 'nf', 'nm', 'nadj', 'prep', 'pro', 'v', 'nmi', 'nfi', 'nmpl', 'nfpl', 'adj\\(f\\)', 'nadj\\(f\\)', 'nm\\(pl\\)', 'nf\\(pl\\)', 'adj\\(pl\\)', 'nadj\\(pl\\)', 'nmfi', \"adjf\", \"nadjpl\"  }\n",
    "    \n",
    "    min = None\n",
    "    max = None\n",
    "    for i in part_of_speach_codes:\n",
    "        part_of_speach_regex =f' {i}[, ]'\n",
    "        match =  regex.search(part_of_speach_regex, str_to_parse,regex.MULTILINE)\n",
    "        if match:\n",
    "            start, end = match.span()\n",
    "            if min is None or start < min:\n",
    "                min = start\n",
    "            if max is None or end > max:\n",
    "                max = end\n",
    "    \n",
    "    if min is None and max is None:\n",
    "        end_of_first_word = str_to_parse.find(' ')\n",
    "        min, max = end_of_first_word, end_of_first_word\n",
    "            \n",
    "    dictionary['french_word'] = str_to_parse[:min].strip()\n",
    "    dictionary['pos_codes'] = str_to_parse[min:max].strip()\n",
    "    dictionary['word_english'] = str_to_parse[max:].split('\\n')[0]\n",
    "    split_sentence = str_to_parse[max:].replace(dictionary['word_english'], \"\", 1).replace('\\n', \"\").split('–')\n",
    "    if len(split_sentence) == 1:\n",
    "        split_sentence =  str_to_parse[max:].replace(dictionary['word_english'], \"\", 1).replace('\\n', \"\").split('-')\n",
    "    sentence_french = split_sentence[0].strip()\n",
    "    sentence_english = \"-\".join(split_sentence[1:]).strip()\n",
    "    dictionary['sentence_french'], dictionary[\"sentence_english\"] = sentence_french, sentence_english\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "failures = []\n",
    "for dict in match_dict:\n",
    "    try:\n",
    "        parse_raw_sentence(dict)\n",
    "    except Exception as e:\n",
    "        failures.append({'frequency_idx': dict['frequency_idx'], 'exception': str(e)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Replace Target Word In Example Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('fr_core_news_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_word_in_sentence(word, sentence):\n",
    "    word_regex = \"(?<=(\\.|,|\\?| |^|'|-))\" + word + \"(?=(\\.|,|\\?| |$||\\!|-))\"\n",
    "    new_sentence = regex.sub(word_regex, \"___\", sentence)\n",
    "    return new_sentence\n",
    "\n",
    "def delete_lemma_from_example(word, sentence):\n",
    "    doc = nlp(sentence)\n",
    "    word_doc = nlp(word)\n",
    "    input_word_token = word_doc[0]\n",
    "    tokens = [{\"original\": token.text, \"lemma\": token.lemma_, 'similarity': token.similarity(input_word_token)} for token in doc]\n",
    "    tokens.sort(key=lambda x: x[\"similarity\"],reverse=True)\n",
    "    for token in tokens:\n",
    "        if token[\"lemma\"] == word or token[\"original\"] == word:\n",
    "            return True, replace_word_in_sentence(token[\"original\"], sentence)\n",
    "    \n",
    "    if len(word_doc) == 1:\n",
    "        return False, replace_word_in_sentence(tokens[0]['original'], sentence)\n",
    "    else:\n",
    "        return False, replace_word_in_sentence(word, sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mw/ts7zr6qn7x1cbg2pvqvp3sh40000gn/T/ipykernel_46689/3212104019.py:10: UserWarning: [W008] Evaluating Token.similarity based on empty vectors.\n",
      "  tokens = [{\"original\": token.text, \"lemma\": token.lemma_, 'similarity': token.similarity(input_word_token)} for token in doc]\n"
     ]
    }
   ],
   "source": [
    "no_lemma_match = []\n",
    "replace_failures = []\n",
    "for item in match_dict:\n",
    "    word = item['french_word']\n",
    "    sentence = item['sentence_french']\n",
    "    success, result = delete_lemma_from_example(word, sentence)\n",
    "\n",
    "    item['sentence_french_deleted'] = result\n",
    "    \n",
    "    if not success:\n",
    "        no_lemma_match.append(item)\n",
    "    if \"_\" not in result:\n",
    "        replace_failures.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in match_dict:\n",
    "    item['pronunciation'] = f\"[sound:french_audio_{item['frequency_idx']}.mp3]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_df = pd.DataFrame(match_dict)\n",
    "export_df.drop(inplace=True, columns='raw_match')\n",
    "export_df.to_csv('basic_french_flashcards.csv', index=False, sep='&')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('french_flashcards')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0a00106c57f919c93c04c3e5bc403e8c74a6df6712ad57de0b788fdb8e17cb9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
